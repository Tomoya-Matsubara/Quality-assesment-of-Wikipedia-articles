


<p>'''Inductive reasoning '''(as opposed to <a href="deductive_reasoning" title="wikilink">''deductive ''reasoning</a>) is reasoning in which the premises seek to supply strong evidence for (not absolute proof of) the truth of the conclusion. While the conclusion of a deductive argument is supposed to be certain, the truth of the conclusion of an inductive argument is supposed to be <em>probable</em>, based upon the evidence given.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>The philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive <a href="logical_argument" title="wikilink">logical argument</a> indicate some degree of support (inductive probability) for the conclusion but do not <a href="entailment" title="wikilink">entail</a> it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below).</p>
<p>Many dictionaries define inductive reasoning as reasoning that derives general principles from specific observations, though some sources disagree with this usage.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<h2 id="description">Description</h2>
<p>Inductive reasoning is inherently <a href="Uncertainty" title="wikilink">uncertain</a>. It only deals in degrees to which, given the premises, the conclusion is <em>credible</em> according to some theory of evidence. Examples include a <a href="many-valued_logic" title="wikilink">many-valued logic</a>, <a href="Dempster&lt;U+2013&gt;Shafer_theory" title="wikilink">Dempster&lt;U+2013&gt;Shafer theory</a>, or <a href="Probability" title="wikilink">probability theory</a> with rules for inference such as <a href="Bayes_rule" title="wikilink">Bayes' rule</a>. Unlike deductive reasoning, it does not rely on universals holding over a <a href="Closed_world_assumption" title="wikilink">closed domain of discourse</a> to draw conclusions, so it can be applicable even in cases of <a href="Open_world_assumption" title="wikilink">epistemic uncertainty</a> (technical issues with this may arise however; for example, the <a href="Axioms_of_probability#Second_axiom" title="wikilink">second axiom of probability</a> is a closed-world assumption).<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>A <a href="statistical_syllogism" title="wikilink">statistical syllogism</a> is an example of inductive reasoning:</p>
<ol>
<li>Almost all people are taller than 26 inches</li>
<li>Gareth is a person</li>
<li>Therefore, Gareth is almost certainly taller than 26 inches</li>
</ol>
<p>As a stronger example:</p>
<dl>

<dd>100% of biological life forms that we know of depend on liquid water to exist.
</dd>
<dd>Therefore, if we discover a new biological life form it will probably depend on liquid water to exist.
</dd>
</dl>
<p>This argument could have been made every time a new biological life form was found, and would have been correct every time; however, it is still possible that in the future a biological life form not requiring water could be discovered.</p>
<p>As a result, the argument may be stated less formally as:</p>
<dl>

<dd>All biological life forms that we know of depend on liquid water to exist.
</dd>
<dd>All biological life probably depends on liquid water to exist.
</dd>
</dl>
<h2 id="inductive-vs.-deductive-reasoning">Inductive vs. deductive reasoning</h2>
<p>Unlike deductive arguments, inductive reasoning allows for the possibility that the conclusion is false, even if all of the premises are true.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Instead of being valid or invalid, inductive arguments are either <em>strong</em> or <em>weak</em>, which describes how <em>probable</em> it is that the conclusion is true.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>A classical example of an <strong>incorrect</strong> inductive argument was presented by John Vickers:</p>
<dl>

<dd>All of the swans we have seen are white.
</dd>
<dd>Therefore, all swans are white.
</dd>
</dl>
<p>Note that this definition of <em>inductive</em> reasoning excludes <a href="mathematical_induction" title="wikilink">mathematical induction</a>, which is a form of <em><a href="deductive_reasoning" title="wikilink">deductive</a></em> reasoning.</p>
<h2 id="criticism">Criticism</h2>

<p>Inductive reasoning has been criticized by thinkers as diverse as <a href="Sextus_Empiricus" title="wikilink">Sextus Empiricus</a><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> and <a href="Karl_Popper" title="wikilink">Karl Popper</a>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The classic philosophical treatment of the <a href="problem_of_induction" title="wikilink">problem of induction</a> was given by the Scottish philosopher <a href="David_Hume" title="wikilink">David Hume</a>.</p>
<p>Although the use of inductive reasoning demonstrates considerable success, its application has been questionable. Recognizing this, Hume highlighted the fact that our mind draws uncertain conclusions from relatively limited experiences. In deduction, the truth value of the conclusion is based on the truth of the premise. In induction, however, the dependence on the premise is always uncertain. As an example, let's assume &quot;all ravens are black.&quot; The fact that there are numerous black ravens supports the assumption. However, the assumption becomes inconsistent with the fact that there are white ravens. Therefore, the general rule of &quot;all ravens are black&quot; is inconsistent with the existence of the white raven. Hume further argued that it is impossible to justify inductive reasoning: specifically, that it cannot be justified deductively, so our only option is to justify it inductively. Since this is circular he concluded that our use of induction is unjustifiable with the help of &quot;Hume's Fork&quot;.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>However, Hume then stated that even if induction were proved unreliable, we would still have to rely on it. So instead of a position of <a href="Philosophical_skepticism" title="wikilink">severe skepticism</a>, Hume advocated a <a href="Scientific_skepticism" title="wikilink">practical skepticism</a> based on <a href="common_sense" title="wikilink">common sense</a>, where the inevitability of induction is accepted.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<h3 id="biases">Biases</h3>
<p>Inductive reasoning is also known as hypothesis construction because any conclusions made are based on current knowledge and predictions. As with deductive arguments, biases can distort the proper application of inductive argument, thereby preventing the reasoner from forming the most <a href="Logical_consequence" title="wikilink">logical conclusion</a> based on the clues. Examples of these biases include the <a href="availability_heuristic" title="wikilink">availability heuristic</a>, <a href="confirmation_bias" title="wikilink">confirmation bias</a>, and the predictable-world bias.</p>
<p>The availability heuristic causes the reasoner to depend primarily upon information that is readily available to him/her. People have a tendency to rely on information that is easily accessible in the world around them. For example, in surveys, when people are asked to estimate the percentage of people who died from various causes, most respondents would choose the causes that have been most prevalent in the media such as terrorism, and murders, and airplane accidents rather than causes such as disease and traffic accidents, which have been technically &quot;less accessible&quot; to the individual since they are not emphasized as heavily in the world around him/her.</p>
<p>The confirmation bias is based on the natural tendency to confirm rather than to deny a current hypothesis. Research has demonstrated that people are inclined to seek solutions to problems that are more consistent with known hypotheses rather than attempt to refute those hypotheses. Often, in experiments, subjects will ask questions that seek answers that fit established hypotheses, thus confirming these hypotheses. For example, if it is hypothesized that Sally is a sociable individual, subjects will naturally seek to confirm the premise by asking questions that would produce answers confirming that Sally is in fact a sociable individual.</p>
<p>The predictable-world bias revolves around the inclination to perceive order where it has not been proved to exist, either at all or at a particular level of abstraction. Gambling, for example, is one of the most popular examples of predictable-world bias. Gamblers often begin to think that they see simple and obvious patterns in the outcomes and, therefore, believe that they are able to predict outcomes based upon what they have witnessed. In reality, however, the outcomes of these games are difficult to predict and highly complex in nature. However, in general, people tend to seek some type of simplistic order to explain or justify their beliefs and experiences, and it is often difficult for them to realise that their perceptions of order may be entirely different from the truth.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<h2 id="types">Types</h2>
<h3 id="generalization">Generalization</h3>
<p>A generalization (more accurately, an <em>inductive generalization</em>) proceeds from a premise about a <a href="statistical_sample" title="wikilink">sample</a> to a conclusion about the <a href="statistical_population" title="wikilink">population</a>.</p>
<dl>

<dd>The proportion Q of the sample has attribute A.
</dd>
<dd>Therefore:
</dd>
<dd>The proportion Q of the population has attribute A.
</dd>
</dl>
<dl>
<dt>Example</dt>

</dl>
<p>There are 20 balls&lt;U+2014&gt;either black or white&lt;U+2014&gt;in an urn. To estimate their respective numbers, you draw a sample of four balls and find that three are black and one is white. A good inductive generalization would be that there are 15 black, and five white, balls in the urn.</p>
<p>How much the premises support the conclusion depends upon (a) the number in the sample group, (b) the number in the population, and (c) the degree to which the sample represents the population (which may be achieved by taking a random sample). The <a href="hasty_generalization" title="wikilink">hasty generalization</a> and the <a href="biased_sample" title="wikilink">biased sample</a> are generalization fallacies.</p>
<h3 id="statistical-syllogism">Statistical syllogism</h3>

<p>A statistical syllogism proceeds from a generalization to a conclusion about an individual.</p>
<dl>

<dd>A proportion Q of population P has attribute A.
</dd>
<dd>An individual X is a member of P.
</dd>
<dd>Therefore:
</dd>
<dd>There is a probability which corresponds to Q that X has A.
</dd>
</dl>
<p>The proportion in the first premise would be something like &quot;3/5ths of&quot;, &quot;all&quot;, &quot;few&quot;, etc. Two <a href="dicto_simpliciter" title="wikilink">dicto simpliciter</a> fallacies can occur in statistical syllogisms: &quot;<a href="accident_(fallacy)" title="wikilink">accident</a>&quot; and &quot;<a href="converse_accident" title="wikilink">converse accident</a>&quot;.</p>
<h3 id="simple-induction">Simple induction</h3>
<p>Simple induction proceeds from a premise about a sample group to a conclusion about another individual.</p>
<dl>

<dd>Proportion Q of the known instances of population P has attribute A.
</dd>
<dd>Individual I is another member of P.
</dd>
<dd>Therefore:
</dd>
<dd>There is a probability corresponding to Q that I has A.
</dd>
</dl>
<p>This is a combination of a generalization and a statistical syllogism, where the conclusion of the generalization is also the first premise of the statistical syllogism.</p>
<h4 id="argument-from-analogy">Argument from analogy</h4>

<p>The process of analogical inference involves noting the shared properties of two or more things, and from this basis inferring that they also share some further property:<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
<dl>

<dd>P and Q are similar in respect to properties a, b, and c.
</dd>
<dd>Object P has been observed to have further property x.
</dd>
<dd>Therefore, Q probably has property x also.
</dd>
</dl>
<p>Analogical reasoning is very frequent in <a href="common_sense" title="wikilink">common sense</a>, <a href="science" class="uri" title="wikilink">science</a>, <a href="philosophy" class="uri" title="wikilink">philosophy</a> and the <a href="humanities" class="uri" title="wikilink">humanities</a>, but sometimes it is accepted only as an auxiliary method. A refined approach is <a href="case-based_reasoning" title="wikilink">case-based reasoning</a>.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<h3 id="causal-inference">Causal inference</h3>
<p>A causal inference draws a conclusion about a causal connection based on the conditions of the occurrence of an effect. Premises about the correlation of two things can indicate a causal relationship between them, but additional factors must be confirmed to establish the exact form of the causal relationship.</p>
<h3 id="prediction">Prediction</h3>
<p>A prediction draws a conclusion about a future individual from a past sample.</p>
<dl>

<dd>Proportion Q of observed members of group G have had attribute A.
</dd>
<dd>Therefore:
</dd>
<dd>There is a probability corresponding to Q that other members of group G will have attribute A when next observed.
</dd>
</dl>
<h2 id="bayesian-inference">Bayesian inference</h2>
<p>As a logic of induction rather than a theory of belief, <a href="Bayesian_inference" title="wikilink">Bayesian inference</a> does not determine which beliefs are <em>a priori</em> rational, but rather determines how we should rationally change the beliefs we have when presented with evidence. We begin by committing to a <a href="prior_probability" title="wikilink">prior probability</a> for a hypothesis based on logic or previous experience, and when faced with evidence, we adjust the strength of our belief in that hypothesis in a precise manner using <a href="conditional_probability" title="wikilink">Bayesian logic</a>.</p>
<h2 id="inductive-inference">Inductive inference</h2>
<p>Around 1960, <a href="Ray_Solomonoff" title="wikilink">Ray Solomonoff</a> founded the theory of universal <a href="Solomonoff&#39;s_theory_of_inductive_inference" title="wikilink">inductive inference</a>, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. This is a formal inductive framework that combines algorithmic information theory with the Bayesian framework. Universal inductive inference is based on solid philosophical foundations<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> and can be considered as a mathematically formalized <a href="Occam&#39;s_razor" title="wikilink">Occam's razor</a>. Fundamental ingredients of the theory are the concepts of <a href="algorithmic_probability" title="wikilink">algorithmic probability</a> and <a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a>.</p>
<h2 id="see-also">See also</h2>

<ul>
<li><a href="Abductive_reasoning" title="wikilink">Abductive reasoning</a></li>
<li><a href="Algorithmic_information_theory" title="wikilink">Algorithmic information theory</a></li>
<li><a href="Algorithmic_probability" title="wikilink">Algorithmic probability</a></li>
<li><a href="Analogy" class="uri" title="wikilink">Analogy</a></li>
<li><a href="Bayesian_probability" title="wikilink">Bayesian probability</a></li>
<li><a href="Counterinduction" class="uri" title="wikilink">Counterinduction</a></li>
<li><a href="Deductive_reasoning" title="wikilink">Deductive reasoning</a></li>
<li><a href="Explanation" class="uri" title="wikilink">Explanation</a></li>
<li><a href="Failure_mode_and_effects_analysis" title="wikilink">Failure mode and effects analysis</a></li>
<li><a href="Falsifiability" class="uri" title="wikilink">Falsifiability</a></li>
<li><a href="Grammar_induction" title="wikilink">Grammar induction</a></li>
<li><a href="Inductive_logic_programming" title="wikilink">Inductive logic programming</a></li>
<li><a href="Inductive_probability" title="wikilink">Inductive probability</a></li>
<li><a href="Inductive_programming" title="wikilink">Inductive programming</a></li>
<li><a href="Inductive_reasoning_aptitude" title="wikilink">Inductive reasoning aptitude</a></li>
<li><a href="Inquiry" class="uri" title="wikilink">Inquiry</a></li>
<li><a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a></li>
<li><a href="Lateral_thinking" title="wikilink">Lateral thinking</a></li>
<li><a href="Laurence_Jonathan_Cohen" title="wikilink">Laurence Jonathan Cohen</a></li>
<li><a href="Logic" class="uri" title="wikilink">Logic</a></li>
<li><a href="Logical_positivism" title="wikilink">Logical positivism</a></li>
<li><a href="Machine_learning" title="wikilink">Machine learning</a></li>
<li><a href="Mathematical_induction" title="wikilink">Mathematical induction</a></li>
<li><a href="Mill&#39;s_Methods" title="wikilink">Mill's Methods</a></li>
<li><a href="Minimum_description_length" title="wikilink">Minimum description length</a></li>
<li><a href="Minimum_message_length" title="wikilink">Minimum message length</a></li>
<li><a href="Open_world_assumption" title="wikilink">Open world assumption</a></li>
<li><a href="Raven_paradox" title="wikilink">Raven paradox</a></li>
<li><a href="Recursive_Bayesian_estimation" title="wikilink">Recursive Bayesian estimation</a></li>
<li><a href="Retroduction" class="uri" title="wikilink">Retroduction</a></li>
<li><a href="Solomonoff&#39;s_theory_of_inductive_inference" title="wikilink">Solomonoff's theory of inductive inference</a></li>
<li><a href="Statistical_inference" title="wikilink">Statistical inference</a></li>
<li><a href="Stephen_Toulmin" title="wikilink">Stephen Toulmin</a></li>
<li><a href="Universal_artificial_intelligence" title="wikilink">Universal artificial intelligence</a></li>
</ul>
<h2 id="references">References</h2>

<h2 id="further-reading">Further reading</h2>
<ul>
<li>Cushan, Anna-Marie (1983/2014). <em>Investigation into Facts and Values: Groundwork for a theory of moral conflict resolution</em>. [Thesis, Melbourne University], Ondwelle Publications (online): Melbourne. <a href="http://www.ondwelle.com/ValueJudgements.pdf">1</a></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>


<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li><a href="http://www.uncg.edu/phi/phi115/induc4.htm"><em>Four Varieties of Inductive Argument</em></a> from the Department of Philosophy, <a href="University_of_North_Carolina_at_Greensboro" title="wikilink">University of North Carolina at Greensboro</a>.</li>
</ul>
<ul>
<li><p>, a psychological review by Evan Heit of the <a href="University_of_California,_Merced" title="wikilink">University of California, Merced</a>.</p></li>
<li><a href="http://dudespaper.com/the-mind-limber.html"><em>The Mind, Limber</em></a> An article which employs the film <a href="The_Big_Lebowski" title="wikilink">The Big Lebowski</a> to explain the value of inductive reasoning.</li>
<li><a href="http://www.academia.edu/4154895/Some_Remarks_on_the_Pragmatic_Problem_of_Induction.html">The Pragmatic Problem of Induction</a>, by Thomas Bullemore</li>
</ul>






<p><a href="Category:Epistemology" class="uri" title="wikilink">Category:Epistemology</a> <a href="Category:Problem_solving" title="wikilink">Category:Problem solving</a> <a href="Category:Inductive_reasoning" title="wikilink"> </a> <a href="Category:Reasoning" class="uri" title="wikilink">Category:Reasoning</a> <a href="Category:Statistical_inference" title="wikilink">Category:Statistical inference</a> <a href="Category:Causal_inference" title="wikilink">Category:Causal inference</a> <a href="Category:Epistemology_of_science" title="wikilink">Category:Epistemology of science</a></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1">Copi, I. M., Cohen, C., &amp; Flage, D. E. (2007). Essentials of logic (2nd ed.). Upper Saddle River, NJ: Pearson Education, Inc.<a href="#fnref1" class="footnote-back">↩</a></li>
<li id="fn2"><a href="#fnref2" class="footnote-back">↩</a></li>
<li id="fn3">Bart Kosko, <em>Fuzziness vs. Probability,</em> International Journal of General Systems, vol. 17, no. 1, pp. 211-240, 1990.<a href="#fnref3" class="footnote-back">↩</a></li>
<li id="fn4">John Vickers. <a href="http://plato.stanford.edu/entries/induction-problem/">The Problem of Induction</a>. The Stanford Encyclopedia of Philosophy.<a href="#fnref4" class="footnote-back">↩</a></li>
<li id="fn5"><a href="#fnref5" class="footnote-back">↩</a></li>
<li id="fn6">Sextus Empiricus, <em>Outlines Of Pyrrhonism</em>. Trans. R.G. Bury, Harvard University Press, Cambridge, Massachusetts, 1933, p. 283.<a href="#fnref6" class="footnote-back">↩</a></li>
<li id="fn7">Karl R. Popper, David W. Miller. &quot;A proof of the impossibility of inductive probability.&quot; <em>Nature</em> 302 (1983), 687&lt;U+2013&gt;688.<a href="#fnref7" class="footnote-back">↩</a></li>
<li id="fn8">Vickers, John. <a href="http://plato.stanford.edu/entries/induction-problem/#2HumIndJus">&quot;The Problem of Induction&quot;</a> (Section 2). <em>Stanford Encyclopedia of Philosophy</em>. 21 June 2010<a href="#fnref8" class="footnote-back">↩</a></li>
<li id="fn9">Vickers, John. <a href="http://plato.stanford.edu/entries/induction-problem/#IndJus">&quot;The Problem of Induction&quot;</a> (Section 2.1). <em>Stanford Encyclopedia of Philosophy</em>. 21 June 2010.<a href="#fnref9" class="footnote-back">↩</a></li>
<li id="fn10">Gray, Peter. Psychology. New York: Worth, 2011. Print.<a href="#fnref10" class="footnote-back">↩</a></li>
<li id="fn11"><a href="#fnref11" class="footnote-back">↩</a></li>
<li id="fn12">For more information on inferences by analogy, see <a href="http://www.cs.hut.fi/Opinnot/T-93.850/2005/Papers/juthe2005-analogy.pdf">Juthe, 2005</a>.<a href="#fnref12" class="footnote-back">↩</a></li>
<li id="fn13">Samuel Rathmanner and <a href="Marcus_Hutter" title="wikilink">Marcus Hutter</a>. A philosophical treatise of universal induction. Entropy, 13(6):1076&lt;U+2013&gt;1136, 2011<a href="#fnref13" class="footnote-back">↩</a></li>
</ol>
</section>
