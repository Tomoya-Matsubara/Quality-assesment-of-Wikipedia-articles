[[File:Handgun collection.JPG|thumb|right|alt=A collection of eight different handguns resting on the ground|Strong opinions on an issue such as [[Gun politics|gun ownership]] can bias how someone interprets new evidence.]]
'''Confirmation bias''' (also called '''confirmatory bias''' or '''myside bias''') is a tendency for people to favor information that confirms their preconceptions or [[hypotheses]], independently of whether they are true.<ref group="Note">[[David Perkins (geneticist)|David Perkins]], a [[geneticist]], coined the term "myside bias" referring to a preference for "my" side of the issue under consideration. {{Harv |Baron|2000|p=195}}</ref><ref name="plous233">{{Harvnb|Plous|1993|p=233}}</ref> This results in people selectively collecting new evidence, interpreting evidence in a [[bias]]ed way, or selectively recalling information from memory. The biases appear in particular for emotionally significant issues and for established beliefs. For example, in reading about [[gun politics|gun control]], people usually prefer sources that affirm their existing attitudes. They also tend to interpret ambiguous evidence as supporting their existing position. Biased search, interpretation and/or recall have been invoked to explain [[attitude polarization]] (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence), belief perseverance (when beliefs persist after the evidence for them is shown to be false), the irrational primacy effect (a stronger weighting for data encountered early in an arbitrary series) and [[illusory correlation]] (in which people falsely perceive an association between two events or situations).

A series of experiments in the 1960s suggested that people are biased towards confirming their existing beliefs. Later work explained these results in terms of a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. In combination with other effects, this strategy can bias the conclusions that are reached. Explanations for the observed biases include [[wishful thinking]] and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong, rather than investigating in a neutral, scientific way.

Confirmation biases contribute to [[overconfidence effect|overconfidence in personal beliefs]] and can maintain or strengthen beliefs in the face of contrary evidence. Hence they can lead to disastrous [[decision making|decisions]], especially in organizational, military and political contexts.

==Types==
Confirmation biases are effects in information processing, distinct from the ''behavioral confirmation effect'', also called "[[self-fulfilling prophecy]]", in which people behave so as to make their expectations come true.<ref>{{Citation |last=Darley |first=John M. |first2=Paget H. |last2=Gross |title=Stereotypes and prejudice: essential readings |editor-first=Charles |editor-last=Stangor |year=2000 |publisher= Psychology Press |page=212 |chapter=A Hypothesis-Confirming Bias in Labelling Effects |isbn=9780863775895 |oclc=42823720}}</ref> Some psychologists use "confirmation bias" to refer to any way in which people avoid rejecting a belief, whether in searching for evidence, interpreting it, or recalling it from memory. Others restrict the term to selective collection of evidence.<ref name="risen">{{Harvnb|Risen|Gilovich|2007}}</ref><ref group="Note">"Assimilation bias" is another term used for biased interpretation of evidence. {{Harv|Risen|Gilovich|2007|p=113}}</ref>

===Biased search for information===
[[File:Fred Barnard07.jpg|thumb|right|alt=A drawing of a man sitting on a stool at a writing desk|Confirmation bias has been described as an internal "[[Wikt:yes man|yes man]]", echoing back a person's thoughts like Charles Dickens' [[Uriah Heep]].<ref name="WSJ">{{Citation |title=How to Ignore the Yes-Man in Your Head |first=Jason |last=Zweig |work=Wall Street Journal |publisher=Dow Jones & Company |date= November 19, 2009 |url=http://online.wsj.com/article/SB10001424052748703811604574533680037778184.html |accessdate=2010-06-13}}</ref>]]
Experiments have repeatedly found that people tend to test hypotheses in a one-sided way, by searching for evidence consistent with the hypothesis they hold at a given time.<ref>{{Harvnb|Nickerson|1998|pp=177<U+2013>178}}</ref><ref name="kunda112"/> Rather than searching through all the relevant evidence, they ask questions that are phrased so that an affirmative answer supports their hypothesis.<ref name="baron162"/> They look for the consequences that they would expect if their hypothesis were true, rather than what would happen if it were false.<ref name="baron162">{{Harvnb|Baron|2000 |pp=162<U+2013>164}}</ref> For example, someone who is trying to identify a number using yes/no questions and suspects that the number is 3 might ask, "Is it an odd number?" People prefer this sort of question, called a "positive test", even when a negative test such as "Is it an even number?" would yield exactly the same information.<ref>{{Harvnb|Kida|2006|pp=162<U+2013>165}}</ref> However, this does not mean that people seek tests that are guaranteed to give a positive answer. In studies where subjects could select either such pseudo-tests or genuinely diagnostic ones, they favored the genuinely diagnostic.<ref>{{Citation |last=Devine |first=Patricia G. |first2=Edward R. |last2=Hirt |first3=Elizabeth M. |last3=Gehrke |year=1990 |title=Diagnostic and confirmation strategies in trait hypothesis testing |journal=Journal of Personality and Social Psychology |publisher=American Psychological Association |volume=58 |issue=6 |pages=952<U+2013>963 |issn=1939-1315 |doi=10.1037/0022-3514.58.6.952}}</ref><ref>{{Citation |last=Trope |first=Yaacov |first2=Miriam |last2=Bassok |year=1982 |title=Confirmatory and diagnosing strategies in social information gathering |journal=Journal of Personality and Social Psychology |publisher=American Psychological Association |volume=43 |issue=1 |pages=22<U+2013>34 |issn=1939-1315 |doi=10.1037/0022-3514.43.1.22}}</ref>

The preference for positive tests is not itself a bias, since positive tests can be highly informative.<ref name="klaymanha"/> However, in conjunction with other effects, this strategy can confirm existing beliefs or assumptions, independently of whether they are true.<ref name="oswald82">{{Harvnb|Oswald|Grosjean|2004|pp=82<U+2013>83}}</ref> In real-world situations, evidence is often complex and mixed. For example, various contradictory ideas about someone could each be supported by concentrating on one aspect of his or her behavior.<ref name="kunda112"/> Thus any search for evidence in favor of a hypothesis is likely to succeed.<ref name=oswald82/> One illustration of this is the way the phrasing of a question can significantly change the answer.<ref name="kunda112">{{Harvnb|Kunda|1999|pp=112<U+2013>115}}</ref> For example, people who are asked, "Are you happy with your social life?" report greater satisfaction than those asked, "Are you ''un''happy with your social life?"<ref>{{Citation |last=Kunda |first=Ziva |first2=G.T. |last2 =Fong |first3=R. |last3=Sanitoso |first4=E. |last4=Reber |year=1993 |title=Directional questions direct self-conceptions |journal=Journal of Experimental Social Psychology |publisher=Society of Experimental Social Psychology |volume=29 |pages=62<U+2013>63 |issn=0022-1031}} via {{Harvnb|Fine|2006|pp=63<U+2013>65}}</ref>

Even a small change in the wording of a question can affect how people search through available information, and hence the conclusions they reach. This was shown using a fictional child custody case.<ref name="shafir"/> Subjects read that Parent A was moderately suitable to be the guardian in multiple ways.  Parent B had a mix of salient positive and negative qualities: a close relationship with the child but a job that would take him or her away for long periods. When asked, "Which parent should have custody of the child?" the subjects looked for positive attributes and a majority chose Parent B. However, when the question was, "Which parent should be denied custody of the child?" they looked for negative attributes, but again a majority answered Parent B, implying that Parent A should have custody.<ref name= "shafir">{{Citation |last=Shafir |first=E. |year=1983 |title=Choosing versus rejecting: why some options are both better and worse than others |journal=Memory and Cognition |volume=21 |pages=546<U+2013>556 |pmid= 8350746 |issue=4}} via {{Harvnb|Fine|2006|pp=63<U+2013>65}}</ref>

Similar studies have demonstrated how people engage in biased search for information, but also that this phenomenon may be limited by a preference for genuine diagnostic tests, where they are available. In an initial experiment, subjects had to rate another person on the [[extraversion and introversion|introversion-extraversion]] personality dimension on the basis of an interview. They chose the interview questions from a given list. When the interviewee was introduced as an introvert, the subjects chose questions that presumed introversion, such as, "What do you find unpleasant about noisy parties?" When the interviewee was described as extraverted, almost all the questions presumed extraversion, such as, "What would you do to liven up a dull party?" These loaded questions gave the interviewees little or no opportunity to falsify the hypothesis about them.<ref>{{Citation |last=Snyder |first=Mark |first2=William B. |last2=Swann, Jr. |year=1978 |title=Hypothesis-Testing Processes in Social Interaction |journal=Journal of Personality and Social Psychology |publisher=American Psychological Association |volume=36 |issue=11 |pages=1202<U+2013>1212 |doi=10.1037/0022-3514.36.11.1202}} via {{Harvnb|Poletiek|2001|p=131}}</ref> However, a later version of the experiment gave the subjects less presumptive questions to choose from, such as, "Do you shy away from social interactions?"<ref name="kunda117"/> Subjects preferred to ask these more diagnostic questions, showing only a weak bias towards positive tests. This pattern, of a main preference for diagnostic tests and a weaker preference for positive tests, has been replicated in other studies.<ref name="kunda117">{{Harvnb|Kunda|1999|pp=117<U+2013>118}}</ref>

Another experiment gave subjects a particularly complex rule-discovery task involving moving objects simulated by a computer.<ref name="mynatt1978">{{Citation |last=Mynatt |first=Clifford R. |first2=Michael E. |last2=Doherty |first3=Ryan D. |last3=Tweney |year=1978 |title=Consequences of confirmation and disconfirmation in a simulated research environment |journal=Quarterly Journal of Experimental Psychology |volume=30 |issue=3 |pages=395<U+2013>406 |doi =10.1080/00335557843000007}}</ref> Objects on the computer screen followed specific laws, which the subjects had to figure out.  They could "fire" objects across the screen to test their hypotheses. Despite making many attempts over a ten hour session, none of the subjects worked out the rules of the system. They typically sought to confirm rather than falsify their hypotheses, and were reluctant to consider alternatives. Even after seeing evidence that objectively refuted their working hypotheses, they frequently continued doing the same tests. Some of the subjects were instructed in proper hypothesis-testing, but these instructions had almost no effect.<ref name= "mynatt1978"/>

===Biased interpretation===
{{Quote box |quote="Smart people believe weird things because they are skilled at defending beliefs they arrived at for non-smart reasons." |source=<U+2014>[[Michael Shermer]]<ref>{{Harvnb|Kida|2006|p=157}}</ref> |width=30% |align=right}}
Confirmation biases are not limited to the collection of evidence. Even if two individuals have the same information, the way they interpret it can be biased.

A team at [[Stanford University]] ran an experiment with subjects who felt strongly about capital punishment, with half in favor and half against.<ref name="lord1979"/><ref name=baron201 /> Each of these subjects read descriptions of two studies; a comparison of [[U.S. state]]s with and without the death penalty, and a comparison of murder rates in a state before and after the introduction of the death penalty. After reading a quick description of each study, the subjects were asked whether their opinions had changed. They then read a much more detailed account of each study's procedure and had to rate how well-conducted and convincing that research was.<ref name="lord1979"/> In fact, the studies were fictional. Half the subjects were told that one kind of study supported the deterrent effect and the other undermined it, while for other subjects the conclusions were swapped.<ref name="lord1979"/><ref name="baron201">{{Harvnb|Baron|2000|pp=201<U+2013>202}}</ref>

The subjects, whether proponents or opponents, reported shifting their attitudes slightly in the direction of the first study they read. Once they read the more detailed descriptions of the two studies, they almost all returned to their original belief regardless of the evidence provided, pointing to details that supported their viewpoint and disregarding anything contrary. Subjects described studies supporting their pre-existing view as superior to those that contradicted it, in detailed and specific ways.<ref name="lord1979"/><ref name ="vyse122">{{Harvnb|Vyse|1997|p=122}}</ref> Writing about a study that seemed to undermine the deterrence effect, a death penalty proponent wrote, "The research didn't cover a long enough period of time", while an opponent's comment on the same study said, "No strong evidence to contradict the researchers has been presented".<ref name="lord1979"/> The results illustrated that people set higher standards of evidence for hypotheses that go against their current expectations. This effect, known as "disconfirmation bias", has been supported by other experiments.<ref name="taber_political"/>

[[File:MRI-Philips.JPG|thumb|right|alt=A large round machine with a hole in the middle, with a platter for a person to lie on so that their head can fit into the hole|An MRI scanner allowed researchers to examine how the human brain deals with unwelcome information.]]
A study of biased interpretation took place during the [[United States presidential election, 2004|2004 US presidential election]], and involved subjects who described themselves as having strong feelings about the candidates. They were shown apparently contradictory pairs of statements, either from Republican candidate [[George W. Bush]], Democratic candidate [[John Kerry]] or a politically neutral public figure. They were also given further statements that made the apparent contradiction seem reasonable. From these three pieces of information, they had to decide whether or not each individual's statements were inconsistent. There were strong differences in these evaluations, with subjects much more likely to interpret statements by the candidate they opposed as contradictory.<ref name="westen2006">{{Citation |last= Westen |first=Drew |first2=Pavel S. |last2=Blagov |first3=Keith |last3=Harenski |first4=Clint |last4=Kilts |first5=Stephan |last5=Hamann |year=2006 |title=Neural Bases of Motivated Reasoning: An fMRI Study of Emotional Constraints on Partisan Political Judgment in the 2004 U.S. Presidential Election |journal=Journal of Cognitive Neuroscience |publisher=Massachusetts Institute of Technology |volume=18 |issue=11 |pages=1947<U+2013>1958 |url=http://psychsystems.net/lab/06_Westen_fmri.pdf |accessdate=2009-08-14 |doi=10.1162/jocn.2006.18.11.1947 |pmid=17069484}}</ref>

In this experiment, the subjects made their judgments while in a [[magnetic resonance imaging]] (MRI) scanner which monitored their brain activity. As subjects evaluated contradictory statements by their favored candidate, [[emotion]]al centers of their brains were aroused. This did not happen with the statements by the other figures. The experimenters' interpretation was that the different responses to the statements were not due to passive reasoning errors. Instead, the subjects were actively reducing the [[cognitive dissonance]] induced by reading about their favored candidate's irrational or hypocritical behavior.<ref name="westen2006"/>

Biased interpretation is not restricted to emotionally significant topics. In another experiment, subjects were told a story about a theft. They had to rate the evidential importance of statements arguing either for or against a particular character being responsible. When they hypothesized that character's guilt, they rated statements supporting that hypothesis as more important than conflicting statements.<ref>{{Citation |last=Gadenne |first=V. |first2=M. |last2=Oswald |year=1986 |title=Entstehung und Ver<U+00E4>nderung von Best<U+00E4>tigungstendenzen beim Testen von Hypothesen [Formation and alteration of confirmatory tendencies during the testing of hypotheses] |journal=Zeitschrift f<U+00FC>r experimentelle und angewandte Psychologie |volume=33 |pages=360<U+2013>374}} via {{Harvnb|Oswald|Grosjean|2004|p=89}}</ref>

===Biased memory===
Even if someone has sought and interpreted evidence in a neutral manner, they may still remember it selectively to reinforce their expectations. This effect is called "selective recall", "confirmatory memory" or "access-biased memory".<ref>{{Citation |last=Hastie |first=Reid |first2=Bernadette |last2=Park |chapter=The Relationship Between Memory and Judgment Depends on Whether the Judgment Task is Memory-Based or On-Line |title=Social cognition: key readings |editor-first=David L. |editor-last=Hamilton |publisher=Psychology Press |location=New York |year=2005 |page=394 |isbn=0863775918 |oclc=55078722}}</ref> Psychological theories differ in their predictions about selective recall. [[Schema (psychology)|Schema theory]] predicts that information matching prior expectations will be more easily stored and recalled.<ref name=oswald88/> Some alternative approaches say that surprising information stands out more and so is more memorable.<ref name="oswald88">{{Harvnb|Oswald|Grosjean|2004|p=88<U+2013>89}}</ref> Predictions from both these theories have been confirmed in different experimental contexts, with no theory winning outright.<ref>{{Citation |last=Stangor |first=Charles |first2=David |last2=McMillan |year=1992 |title= Memory for expectancy-congruent and expectancy-incongruent information: A review of the social and social developmental literatures |journal=Psychological Bulletin |publisher=American Psychological Association |volume=111 |issue=1 |pages=42<U+2013>61 |doi=10.1037/0033-2909.111.1.42}}</ref>

In one study, subjects read a profile of a woman which described a mix of introverted and extraverted behaviors.<ref name="snydercantor"/> They later had to recall examples of her introversion and extraversion. One group was told this was to assess the woman for a job as a librarian, while a second group were told it was for a job in real estate sales. There was a significant difference between what these two groups recalled, with the "librarian" group recalling more examples of introversion and the "sales" groups recalling more extraverted behavior.<ref name="snydercantor">{{Citation |last= Snyder |first=M. |first2=N. |last2=Cantor |year=1979 |title=Testing hypotheses about other people: the use of historical knowledge |journal=Journal of Experimental Social Psychology |volume=15 |pages=330<U+2013>342 |doi=10.1016/0022-1031(79)90042-8}} via {{Harvnb|Goldacre|2008|p=231}}</ref> A selective memory effect has also been shown in experiments that manipulate the desirability of personality types.<ref name=oswald88 /><ref>{{Harvnb|Kunda|1999|pp=225<U+2013>232}}</ref> In one of these, a group of subjects were shown evidence that extraverted people are more successful than introverts. Another group were told the opposite. In a subsequent, apparently unrelated, study, they were asked to recall events from their lives in which they had been either introverted or extraverted. Each group of subjects provided more memories connecting themselves with the more desirable personality type, and recalled those memories more quickly.<ref>{{Citation |last=Sanitioso |first=Rasyid |first2=Ziva |last2=Kunda |first3=G.T. |last3=Fong |year=1990 |title= Motivated recruitment of autobiographical memories |journal=Journal of Personality and Social Psychology |publisher=American Psychological Association |issn=0022-3514 |volume=59 |issue=2 |pages=229<U+2013>241 |doi= 10.1037/0022-3514.59.2.229 |pmid=2213492}}</ref>

One study showed how selective memory can maintain belief in [[extrasensory perception]] (ESP).<ref name="russell_jones">{{Citation |last=Russell |first=Dan |first2=Warren H. |last2=Jones |year=1980 |title=When superstition fails: Reactions to disconfirmation of paranormal beliefs |journal=Personality and Social Psychology Bulletin |publisher=Society for Personality and Social Psychology |volume=6 |issue=1 |pages= 83<U+2013>88 |issn=1552-7433 |doi=10.1177/014616728061012}} via {{Harvnb|Vyse|1997|p=121}}</ref> Believers and disbelievers were each shown descriptions of ESP experiments. Half of each group were told that the experimental results supported the existence of ESP, while the others were told they did not. In a subsequent test, subjects recalled the material accurately, apart from believers who had read the non-supportive evidence. This group remembered significantly less information and some of them incorrectly remembered the results as supporting ESP.<ref name="russell_jones"/>

==Related effects==
===Polarization of opinion===
{{main|Attitude polarization}}
When people with opposing views interpret new information in a biased way, their views can move even further apart. This is called "attitude polarization".<ref name="kuhn_lao"/> The effect was demonstrated by an experiment that involved drawing a series of red and black balls from one of two concealed "bingo baskets". Subjects knew that one basket contained 60% black and 40% red balls; the other, 40% black and 60% red. The experimenters looked at what happened when balls of alternating color were drawn in turn, a sequence that does not favor either basket. After each ball was drawn, subjects in one group were asked to state out loud their judgments of the probability that the balls were being drawn from one or the other basket. These subjects tended to grow more confident with each successive draw<U+2014>whether they initially thought the basket with 60% black balls or the one with 60% red balls was the more likely source, their estimate of the probability increased. Another group of subjects were asked to state probability estimates only at the end of a sequence of drawn balls, rather than after each ball. They did not show the polarization effect, suggesting that it does not necessarily occur when people simply hold opposing positions, but rather when they openly commit to them.<ref>{{Harvnb|Baron|2000|p=201}}</ref>

A less abstract study was the Stanford biased interpretation experiment in which subjects with strong opinions about the death penalty read about mixed experimental evidence. Twenty-three percent of the subjects reported that their views had become more extreme, and this self-reported shift [[correlation|correlated]] strongly with their initial attitudes.<ref name="lord1979">{{Citation |last=Lord |first=Charles G. |first2=Lee |last2 =Ross |first3=Mark R. |last3=Lepper |year=1979 |title=Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence |journal=Journal of Personality and Social Psychology |publisher=American Psychological Association |volume=37 |issue=11 |pages=2098<U+2013>2109 |issn=0022-3514 |doi=10.1037/0022-3514.37.11.2098}}</ref> In later experiments, subjects also reported their opinions becoming more extreme in response to ambiguous information. However, comparisons of their attitudes before and after the new evidence showed no significant change, suggesting that the self-reported changes might not be real.<ref name="taber_political">{{Citation |last=Taber |first=Charles S. |first2=Milton |last2=Lodge |date=July 2006 |title=Motivated Skepticism in the Evaluation of Political Beliefs |journal=American Journal of Political Science |publisher=Midwest Political Science Association |volume=50 |issue=3 |pages=755<U+2013>769 |issn=0092-5853 |doi=10.1111/j.1540-5907.2006.00214.x}}</ref><ref name= "kuhn_lao">{{Citation |last=Kuhn |first=Deanna |first2=Joseph |last2=Lao |date=March 1996 |title=Effects of Evidence on Attitudes: Is Polarization the Norm? |journal=Psychological Science |publisher=American Psychological Society |volume=7 |issue=2 |pages=115<U+2013>120 |doi=10.1111/j.1467-9280.1996.tb00340.x}}</ref><ref>{{Citation |last=Miller |first=A.G.|first2=J.W. |last2=McHoskey |first3=C.M. |last3=Bane |first4=T.G. |last4=Dowd |year=1993 |title=The attitude polarization phenomenon: Role of response measure, attitude extremity, and behavioral consequences of reported attitude change |journal=Journal of Personality and Social Psychology |volume=64 |pages=561<U+2013>574 |doi=10.1037/0022-3514.64.4.561}}</ref> Based on these experiments, Deanna Kuhn and Joseph Lao concluded that polarization is a real phenomenon but far from inevitable, only happening in a small minority of cases. They found that it was prompted not only by considering mixed evidence, but by merely thinking about the topic.<ref name="kuhn_lao"/>

Charles Taber and Milton Lodge argued that the Stanford team's result had been hard to replicate because the arguments used in later experiments were too abstract or confusing to evoke an emotional response. The Taber and Lodge study used the emotionally charged topics of [[gun politics|gun control]] and [[affirmative action]].<ref name="taber_political"/> They measured the attitudes of their subjects towards these issues before and after reading arguments on each side of the debate. Two groups of subjects showed attitude polarization; those with strong prior opinions and those who were politically knowledgeable. In part of this study, subjects chose which information sources to read, from a list prepared by the experimenters. For example they could read the [[National Rifle Association]]'s and the [[Brady Campaign|Brady Anti-Handgun Coalition]]'s arguments on gun control. Even when instructed to be even-handed, subjects were more likely to read arguments that supported their existing attitudes. This biased search for information correlated well with the polarization effect.<ref name="taber_political"/>

===Persistence of discredited beliefs===
{{Quote box |quote="[B]eliefs can survive potent logical or empirical challenges. They can survive and even be bolstered by evidence that most uncommitted observers would agree logically demands some weakening of such beliefs. They can even survive the total destruction of their original evidential bases." |source=<U+2014>Lee Ross and Craig Anderson<ref name="shortcomings"/> |width=30% |align=right}}
Confirmation biases can be used to explain why some beliefs remain when the initial evidence for them is removed.<ref name="shortcomings"/> This belief perseverance effect has been shown by a series of experiments using what is called the "debriefing paradigm": subjects examine faked evidence for a hypothesis, their attitude change is measured, then they learn that the evidence was fictitious. Their attitudes are then measured once more to see if their belief returns to its previous level.<ref name="shortcomings">{{Citation |last=Ross |first=Lee |first2=Craig A. |last2=Anderson |title=Judgment under uncertainty: Heuristics and biases |editor1-first=Daniel |editor1-last=Kahneman |editor2-first=Paul |editor2-last=Slovic |editor3-first=Amos |editor3-last=Tversky |publisher=Cambridge University Press |year=1982 |pages= 129<U+2013>152 |chapter=Shortcomings in the attribution process: On the origins and maintenance of erroneous social assessments |isbn=9780521284141 |oclc=7578020}}</ref>

A typical finding is that at least some of the initial belief remains even after a full debrief.<ref name="kunda99">{{Harvnb|Kunda|1999|p=99}}</ref> In one experiment, subjects had to distinguish between real and fake suicide notes. The feedback was random: some were told they had done well while others were told they had performed badly. Even after being fully debriefed, subjects were still influenced by the feedback. They still thought they were better or worse than average at that kind of task, depending on what they had initially been told.<ref>{{Citation |last=Ross |first=Lee |first2=Mark R. |last2=Lepper |first3=Michael |last3=Hubbard |title=Perseverance in self-perception and social perception: Biased attributional processes in the debriefing paradigm |journal=Journal of Personality and Social Psychology |volume=32 |publisher=American Psychological Association |issn=0022-3514 |pages=880<U+2013>892 |issue=5 |year=1975 |doi=10.1037/0022-3514.32.5.880 |pmid=1185517}} via {{Harvnb|Kunda|1999|p=99}}</ref>

In another study, subjects read job performance ratings of two firefighters, along with their responses to a [[risk aversion]] test.<ref name="shortcomings"/> These fictional data were arranged to show either a negative or positive [[correlation|association]] between risk-taking attitudes and job success.<ref name="socialperseverance"/> Even if these case studies had been true, they would have been scientifically poor evidence. However, the subjects found them subjectively persuasive.<ref name="socialperseverance">{{Citation |title=Perseverance of Social Theories: The Role of Explanation in the Persistence of Discredited Information |first=Craig A. |last=Anderson |first2=Mark R. |last2=Lepper |first3=Lee |last3=Ross |journal=Journal of Personality and Social Psychology |year=1980 |volume=39 |issue=6 |pages=1037<U+2013>1049 |publisher= American Psychological Association |issn=0022-3514 |doi=10.1037/h0077720}}</ref> When the case studies were shown to be fictional, subjects' belief in a link diminished, but around half of the original effect remained.<ref name="shortcomings"/> Follow-up interviews established that the subjects had understood the debriefing and taken it seriously. Subjects seemed to trust the debriefing, but regarded the discredited information as irrelevant to their personal belief.<ref name="socialperseverance"/>

===Preference for early information===
Experiments have shown that information is weighted more strongly when it appears early in a series, even when the order is unimportant. For example, people form a more positive impression of someone described as, "intelligent, industrious, impulsive, critical, stubborn, envious", than when they are given the same words in reverse order.<ref name="baron197">{{Harvnb|Baron|2000|pp=197<U+2013>200}}</ref> This irrational primacy effect is independent of the [[serial position effect|primacy effect in memory]] in which the earlier items in a series leave a stronger memory trace.<ref name="baron197"/> Biased interpretation offers an explanation for this effect: seeing the initial evidence, people form a working hypothesis that affects how they interpret the rest of the information.<ref name="nick187">{{Harvnb |Nickerson|1998|p=187}}</ref>

One demonstration of irrational primacy involved colored chips supposedly drawn from two urns. Subjects were told the color distributions of the urns, and had to estimate the probability of a chip being drawn from one of them.<ref name="baron197"/> In fact, the colors appeared in a pre-arranged order. The first thirty draws favored one urn and the next thirty favored the other.<ref name=nick187/> The series as a whole was neutral, so rationally, the two urns were equally likely. However, after sixty draws, subjects favored the urn suggested by the initial thirty.<ref name="baron197"/> Another experiment involved a slide show of a single object, seen as just a blur at first and in slightly better focus with each succeeding slide.<ref name="baron197"/> After each slide, subjects had to state their best guess of what the object was. Subjects whose early guesses were wrong persisted with those guesses, even when the picture was sufficiently in focus that other people could readily identify the object.<ref name="nick187"/>

===Illusory association between events===
{{main|Illusory correlation}}
[[Illusory correlation]] is the tendency to see non-existent correlations in a set of data that fit one's preconceptions.<ref name="fine">{{Harvnb|Fine|2006|pp=66<U+2013>70}}</ref> This phenomenon was first demonstrated in a 1969 experiment involving the [[Rorschach inkblot test]]. Subjects read a set of psychiatric case studies, and reported that the homosexual men in the set were more likely to report seeing buttocks or anuses in the ambiguous figures. In fact the case studies were fictional and, in one version of the experiment, had been constructed so that the homosexual men were less likely to report this imagery.<ref name="fine"/> Another study recorded the symptoms experienced by arthritic patients, along with weather conditions over a 15-month period. Nearly all the patients reported that their pains were correlated with weather conditions, although the real correlation was zero.<ref>{{Citation |last=Redelmeir |first=D. A. |first2=Amos |last2=Tversky |year=1996 |title=On the belief that arthritis pain is related to the weather |journal=Proceedings of the National Academy of Science |volume=93 |pages=2895<U+2013>2896 |doi=10.1073/pnas.93.7.2895}} via {{Harvnb|Kunda|1999|p=127}}</ref>

This effect is a kind of biased interpretation, in that objectively neutral or unfavorable evidence is interpreted to support existing beliefs. It is also related to biases in hypothesis-testing behavior.<ref name="kunda127">{{Harvnb|Kunda|1999|pp=127<U+2013>130}}</ref> In judging whether two events, such as illness and bad weather, are correlated, people rely heavily on the number of ''positive-positive'' cases: in this example, instances of both pain and bad weather. They pay relatively little attention to the other kinds of observation (of no pain and/or good weather).<ref>{{Harvnb|Plous|1993|pp=162<U+2013>164}}</ref> This parallels the reliance on positive tests in hypothesis testing.<ref name="kunda127"/> It may also reflect selective recall, in that people may have a sense that two events are correlated because it is easier to recall times when they happened together.<ref name="kunda127"/>

{| class="wikitable" border="1" cellpadding="4" style="width:250px;text-align:center;margin: 1em auto 1em auto"
|+ Example
|-
! Days !! Rain !! No rain
|-
! Arthritis
| 14 || 6
|-
! No arthritis
| 7 || 2 
|}
In the above fictional example, there is actually a slight negative correlation between rain and arthritis symptoms, considering all four cells of the table. However, people are likely to focus on the relatively large number in the top-left cell, representing days with both rain and arthritic symptoms, and think they see a positive association.<ref>Adapted from {{Citation |last=Fielder |first=Klaus |title= Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory |editor-last=Pohl |editor-first=R<U+00FC>diger F. |publisher=Psychology Press |location=Hove, UK |year=2004 |chapter=Illusory correlation |isbn=9781841693514 |oclc=55124398 |page=103}}</ref>

==History==
[[Image:Francis Bacon.jpg|thumb|upright|right|alt=Engraved head-and-shoulders portrait of Francis Bacon wearing a hat and ruff. |[[Francis Bacon]]]]

===Informal observation===
Before psychological research on confirmation bias, the phenomenon had been observed anecdotally by writers including the Greek historian [[Thucydides]] (c. 460 BC <U+2013> c. 395 BC),  English philosopher and scientist [[Francis Bacon]] (1561<U+2013>1626)<ref name= "baron195">{{Harvnb|Baron|2000|pp=195<U+2013>196}}</ref> and Russian author [[Leo Tolstoy]] (1828<U+2013>1910). Thucydides, in the ''[[History of the Peloponnesian War]]'' wrote, "it is a habit of mankind (...) to use sovereign reason to thrust aside what they do not fancy."<ref>{{Citation |author=Thucydides |date=431 BCE |publisher=The Internet Classics Archive |author2=Crawley, Richard (trans) |title=The History of the Peloponnesian War |url=http://classics.mit.edu/Thucydides/pelopwar.mb.txt |chapter=XIV |accessdate= 2010-05-27}}</ref> Bacon, in the ''[[Novum Organum]]'' wrote,
{{quote|The human understanding when it has once adopted an opinion (...)<!--"(either as being the received opinion or as being agreeable to itself)" omitted for space--> draws all things else to support and agree with it. And though there be a greater number and weight of instances to be found on the other side, yet these it either neglects or despises, or else by some distinction sets aside or rejects[.]<ref name ="bacon"/>}}

Bacon said that biased assessment of evidence drove "all superstitions, whether in astrology, dreams, omens, divine judgments or the like".<ref name="bacon">Bacon, Francis (1620). ''Novum Organum''. reprinted in {{Citation |title=The English philosophers from Bacon to Mill |editor-first=E.A. |editor-last=Burtt |publisher=Random House |location=New York |year=1939 |page=36}} via {{Harvnb|Nickerson|1998|p=176}}</ref>

===Wason's research on hypothesis-testing===
The term "confirmation bias" was coined by English psychologist [[Peter Cathcart Wason|Peter Wason]].<ref>{{Citation |last=Gale |first=Maggie |first2=Linden J. |last2=Ball |year=2002 |chapter=Does Positivity Bias Explain Patterns of Performance on Wason's 2-4-6 task? |editor1-first=Wayne D. |editor1-last=Gray |editor2-first=Christian D. |editor2-last=Schunn |title=Proceedings of the Twenty-Fourth Annual Conference of the Cognitive Science Society |publisher=Routledge |isbn=9780805845815 |oclc=469971634 |page=340}}</ref> For an experiment published in 1960, he challenged subjects to identify a rule applying to [[tuple|triple]]s of numbers. At the outset, they were told that (2,4,6) fits the rule. Subjects could generate their own triples and the experimenter told them whether or not each triple conformed to the rule.<ref name="wason1960"/><ref>{{Harvnb|Nickerson |1998|p=179}}</ref>

While the actual rule was simply "any ascending sequence", the subjects had a great deal of difficulty in arriving at it, often announcing rules that were far more specific, such as "the middle number is the average of the first and last".<ref name="wason1960">{{Citation |last=Wason |first=Peter C. |year=1960 |title=On the failure to eliminate hypotheses in a conceptual task |journal=Quarterly Journal of Experimental Psychology |volume=12 |issue=3 |publisher=Psychology Press |issn=1747-0226 |pages=129<U+2013>140 |doi=10.1080/17470216008416717}}</ref> The subjects seemed to test only positive examples<U+2014>triples that obeyed their hypothesized rule. For example, if they thought the rule was, "Each number is two greater than its predecessor", they would offer a triple that fit this rule, such as (11,13,15) rather than a triple that violates it, such as (11,12,19).<ref>{{Harvnb|Lewicka|1998|page=238}}</ref>

Wason accepted [[falsificationism]], according to which a scientific test of a hypothesis is a serious attempt to falsify it. He interpreted his results as showing a preference for confirmation over falsification, hence the term "confirmation bias".<ref group="Note">Wason also used the term "verification bias". {{Harv|Poletiek|2001|p=73}}</ref><ref name="oswald">{{Harvnb|Oswald|Grosjean|2004|pp=79<U+2013>96}}</ref> Wason also used confirmation bias to explain the results of his [[Wason selection task|selection task]] experiment.<ref>{{Citation |last=Wason |first=Peter C. |year=1968 |title=Reasoning about a rule |journal=Quarterly Journal of Experimental Psychology |publisher=Psychology Press |issn= 1747-0226 |volume=20 |issue=3 |pages=273<U+2013>28 |doi=10.1080/14640746808400161}}</ref> In this task, subjects are given partial information about a set of objects, and have to specify what further information they would need to tell whether or not a [[material conditional|conditional rule]] ("If A, then B") applies. It has been found repeatedly that people perform badly on various forms of this test, in most cases ignoring information that could potentially refute the rule.<ref name="sutherland"/><ref>{{Citation |last=Barkow |first=Jerome H. |first2=Leda |last2=Cosmides |first3=John |last3=Tooby |title=The adapted mind: evolutionary psychology and the generation of culture |publisher=Oxford University Press US |year=1995 |pages=181<U+2013>184 |isbn=9780195101072 |oclc=33832963}}</ref>

===Klayman and Ha's critique===
A 1987 paper by Joshua Klayman and Young-Won Ha argued that the Wason experiments had not actually demonstrated a bias towards confirmation. Instead, Klayman and Ha interpreted the results in terms of a tendency to make tests that are consistent with the working hypothesis.<ref>{{Harvnb|Oswald|Grosjean|2004|pp=81<U+2013>82,86-87}}</ref> They called this the "positive test strategy".<ref name=kunda112 /> This strategy is an example of a [[heuristic]]: a reasoning shortcut that is imperfect but easy to compute.<ref name=plous233 /> Klayman and Ha used [[Bayesian probability]] and [[information theory]] as their standard of hypothesis-testing, rather than the falsificationism used by Wason. According to these ideas, each answer to a question yields a different amount of information, which depends on the person's prior beliefs. Thus a scientific test of a hypothesis is one that is expected to produce the most information. Since the information content depends on initial probabilities, a positive test can either be highly informative or uninformative. Klayman and Ha argued that when people think about realistic problems, they are looking for a specific answer with a small initial probability. In this case, positive tests are usually more informative than negative tests.<ref name="klaymanha">{{Citation |last=Klayman |first= Joshua |first2=Young-Won |last2=Ha |year=1987 |title=Confirmation, Disconfirmation and Information in Hypothesis Testing |journal=Psychological Review |publisher=American Psychological Association |volume=94 |issue=2 |pages=211<U+2013>228 |issn=0033-295X |url=http://www.stats.org.uk/statistical-inference/KlaymanHa1987.pdf |accessdate=2009-08-14 |doi=10.1037/0033-295X.94.2.211}}</ref> However, in Wason's rule discovery task the answer<U+2014>three numbers in ascending order<U+2014>is very broad, so positive tests are unlikely to yield informative answers. Klayman and Ha supported their analysis by citing an experiment that used the labels "DAX" and "MED" in place of "fits the rule" and "doesn't fit the rule". This avoided implying that the aim was to find a low-probability rule. Subjects had much more success with this version of the experiment.<ref>{{Harvnb|Lewicka|1998|page=239}}</ref><ref>{{Citation |last=Tweney |first=Ryan D. |first2=Michael E. |last2=Doherty |first3=Winifred J. |last3=Worner |first4=Daniel B. |last4=Pliske |first5=Clifford R. |last5=Mynatt |first6=Kimberly A. |last6=Gross |first7=Daniel L. |last7= Arkkelin |year=1980 |title=Strategies of rule discovery in an inference task |journal=The Quarterly Journal of Experimental Psychology |publisher=Psychology Press |issn=1747-0226 |volume=32 |issue=1 |pages= 109<U+2013>123 |doi=10.1080/00335558008248237}} (Experiment IV)</ref>
{|
|-valign="top"
| [[Image:Klayman Ha1.svg|thumb|alt=Within the universe of all possible triples, those that fit the true rule are shown schematically as a circle. The hypothesized rule is a smaller circle enclosed within it. |If the true rule (T) encompasses the current hypothesis (H), then positive tests (examining an H to see if it is T) will not show that the hypothesis is false.]]
| [[Image:Klayman Ha2.svg|thumb|alt=Two overlapping circles represent the true rule and the hypothesized rule. Any observation falling in the non-overlapping parts of the circles shows that the two rules are not exactly the same. In other words, those observations falsify the hypothesis.|If the true rule (T) ''overlaps'' the current hypothesis (H), then either a negative test or a positive test can potentially falsify H.]]
| [[Image:Klayman ha3 annotations.svg|thumb|alt=The triples fitting the hypothesis are represented as a circle within the universe of all triples. The true tule is a smaller circle within this.|When the working hypothesis (H) includes the true rule (T) then positive tests are the ''only'' way to falsify H.]]
|}
In light of this and other critiques, the focus of research moved away from confirmation versus falsification to examine whether people test hypotheses in an informative way, or an uninformative but positive way. The search for "true" confirmation bias led psychologists to look at a wider range of effects in how people process information.<ref>{{Harvnb|Oswald|Grosjean|2004|pp=86<U+2013>89}}</ref>

==Explanations==
Cognitive explanations for confirmation bias are based on limitations in people's ability to handle complex tasks, and the shortcuts, called "heuristics", that they use.<ref>{{Harvnb|Friedrich|1993|p=298}}</ref> For example, people may judge the reliability of evidence by using the ''[[availability heuristic]]'', i.e. how readily a particular idea comes to mind.<ref>{{Harvnb|Kunda|1999|p=94}}</ref> It is also possible that people can only focus on one thought at a time, so find it difficult to test alternative hypotheses in parallel.<ref>{{Harvnb|Nickerson|1998|pp=198<U+2013>199}}</ref> Another heuristic is the positive test strategy identified by Klayman and Ha, in which people test a hypothesis by examining cases where they expect a property or event to occur. This heuristic avoids the difficult or impossible task of working out how diagnostic each possible question will be. However, it is not universally reliable, so people can overlook challenges to their existing beliefs.<ref name="klaymanha"/><ref>{{Harvnb|Nickerson|1998|p=200}}</ref>

Motivational explanations involve an effect of [[desire (emotion)|desire]] on [[belief]], sometimes called "[[wishful thinking]]".<ref name=nick197/><ref>{{Harvnb|Baron|2000|p=206}}</ref> It is known that people prefer pleasant thoughts over unpleasant ones in a number of ways: this is called the "[[Pollyanna principle]]".<ref>{{Citation |last=Matlin |first=Margaret W. |title=Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory |editor-first=R<U+00FC>diger F. |editor-last=Pohl |publisher=Psychology Press |location=Hove |year=2004 |pages=255<U+2013>272 |chapter=Pollyanna Principle |isbn= 9781841693514 |oclc=55124398}}</ref> Applied to [[argument]]s or sources of [[evidence]], this could explain why desired conclusions are more likely to be believed true.<ref name="nick197">{{Harvnb|Nickerson |1998|p=197}}</ref> According to experiments that manipulate the desirability of the conclusion, people demand a high standard of evidence for unpalatable ideas and a low standard for preferred ideas. In other words, they ask, "Can I believe this?" for some suggestions and, "Must I believe this?" for others.<ref>{{Citation |last=Dawson |first=Erica |first2=Thomas |last2=Gilovich |first3=Dennis T. |last3=Regan |date= October 2002 |title=Motivated Reasoning and Performance on the Wason Selection Task |journal=Personality and Social Psychology Bulletin |publisher=Society for Personality and Social Psychology |volume=28 |issue =10 |pages=1379<U+2013>1387 |url=http://comp9.psych.cornell.edu/sec/pubPeople/tdg1/Dawson.Gilo.Regan.pdf |accessdate=2009-09-30 |doi=10.1177/014616702236869}}</ref><ref>{{Citation |last=Ditto |first=Peter H. |first2= David F. |last2=Lopez |year=1992 |title=Motivated skepticism: use of differential decision criteria for preferred and nonpreferred conclusions |journal=Journal of personality and social psychology |publisher= American Psychological Association |volume=63 |issue=4 |pages=568<U+2013>584 |issn=0022-3514 |doi=10.1037/0022-3514.63.4.568}}</ref> Although [[consistency]] is a desirable feature of attitudes, an excessive drive for consistency is another potential source of bias because it may prevent people from neutrally evaluating new, surprising information.<ref name=nick197/> Social psychologist [[Ziva Kunda]] combines the cognitive and motivational theories, arguing that motivation creates the bias, but cognitive factors determine the size of the effect.<ref>{{Harvnb|Nickerson|1998|p=198}}</ref>

Explanations in terms of [[cost-benefit analysis]] assume that people do not just test hypotheses in a disinterested way, but assess the costs of different errors.<ref>{{Harvnb|Oswald|Grosjean|2004|pp=91<U+2013>93}}</ref> Using ideas from [[evolutionary psychology]], James Friedrich suggests that people do not primarily aim at [[truth]] in testing hypotheses, but try to avoid the most costly errors. For example, employers might ask one-sided questions in job interviews because they are focused on weeding out unsuitable candidates.<ref>{{Harvnb|Friedrich|1993|pp=299, 316<U+2013>317}}</ref> Yaacov Trope and Akiva Liberman's refinement of this theory assumes that people compare the two different kinds of error: accepting a false hypothesis or rejecting a true hypothesis. For instance, someone who underestimates a friend's honesty might treat him or her suspiciously and so undermine the friendship. Overestimating the friend's honesty may also be costly, but less so. In this case, it would be rational to seek, evaluate or remember evidence of their honesty in a biased way.<ref>{{Citation |last=Trope |first=Y. |first2=A. |last2=Liberman |title=Social Psychology: Handbook of basic principles |editor1-first=E. Tory |editor1-last=Higgins |editor2-first=Arie W. |editor2-last=Kruglanski |publisher=Guilford Press |location=New York |year=1996 |chapter=Social hypothesis testing: cognitive and motivational mechanisms |isbn=9781572301009 |oclc=34731629}} via {{Harvnb |Oswald|Grosjean|2004|pp=91<U+2013>93}}</ref> When someone gives an initial impression of being introverted or extraverted, questions that match that impression come across as more [[empathic]].<ref name=dardenne/> This suggests that when talking to someone who seems to be an introvert, it is a sign of better [[social skill]]s to ask, "Do you feel awkward in social situations?" rather than, "Do you like noisy parties?" The connection between confirmation bias and social skills was corroborated by a study of how college students get to know other people. Highly [[self-monitoring]] students, who are more sensitive to their environment and to [[social norms]], asked more matching questions when interviewing a high-status staff member than when getting to know fellow students.<ref name="dardenne">{{Citation |last=Dardenne |first=Benoit |first2=Jacques-Philippe |last2=Leyens |title=Confirmation Bias as a Social Skill |journal=Personality and Social Psychology Bulletin |year=1995 |volume=21 |issue=11 |pages=1229<U+2013>1239 |doi=10.1177/01461672952111011 |publisher=Society for Personality and Social Psychology |issn=1552-7433}}</ref>

==Consequences==
===In finance===
Confirmation bias can lead investors to be overconfident, ignoring evidence that their strategies will lose money.<ref name=WSJ/><ref>{{Citation |title=Behavioral finance and wealth management: how to build optimal portfolios that account for investor biases |first=Michael M. |last=Pompian |publisher=John Wiley and Sons |isbn=9780471745174 |oclc=61864118 |year=2006 |pages=187<U+2013>190}}</ref> In studies of [[election stock market|political stock markets]], investors made more profit when they resisted bias. For example, participants who interpreted a candidate's debate performance in a neutral rather than partisan way were more likely to profit.<ref>{{Citation |last=Hilton |first=Denis J. |journal=Journal of Behavioral Finance |year=2001 |title=The psychology of financial decision-making: Applications to trading, dealing, and investment analysis |volume=2 |issue=1 |doi=10.1207/S15327760JPFM0201_4 |publisher=Institute of Behavioral Finance |issn=1542-7579 |pages=37<U+2013>39}}</ref> To combat the effect of confirmation bias, investors can try to adopt a contrary viewpoint "for the sake of argument".<ref>{{Citation |first=David |last= Krueger |first2=John David |last2=Mann |title=The Secret Language of Money: How to Make Smarter Financial Decisions and Live a Richer Life |isbn=9780071623391 |oclc=277205993 |year=2009 |publisher=McGraw Hill Professional |pages=112<U+2013>113}}</ref> One such technique involves imagining that their investments have collapsed and asking why this might happen.<ref name=WSJ/>

===In physical and mental health===
Raymond Nickerson, a psychologist, blames confirmation bias for the ineffective medical procedures that were used for centuries before the [[History of medicine|arrival of scientific medicine]].<ref name= "nick192">{{Harvnb|Nickerson|1998|p=192}}</ref> If a patient recovered, medical authorities counted the treatment as successful, rather than looking for alternative explanations such as that the disease had run its natural course.<ref name=nick192/> Biased assimilation is a factor in the modern appeal of [[alternative medicine]], whose proponents are swayed by positive [[anecdotal evidence]] but treat scientific evidence hyper-critically.<ref>{{Harvnb|Goldacre|2008|p=233}}</ref><ref>{{Citation |last=Singh |first=Simon |first2=Edzard |last2=Ernst |title=Trick or Treatment?: Alternative Medicine on Trial |publisher= Bantam |location=London |year=2008 |isbn=9780593061299 |pages=287<U+2013>288}}</ref><ref>{{Citation |last=Atwood |first=Kimball |year=2004 |title=Naturopathy, Pseudoscience, and Medicine: Myths and Fallacies vs Truth |journal=Medscape General Medicine |volume=6 |issue=1 |page=33}}</ref>

[[Cognitive therapy]] was developed by [[Aaron T. Beck]] in the early 1960s and has become a popular approach.<ref>{{Citation |first=Michael |last=Neenan |first2=Windy |last2=Dryden |year=2004 |title=Cognitive therapy: 100 key points and techniques |publisher=Psychology Press |isbn=9781583918586 |oclc=474568621 |page=ix}}</ref> According to Beck, biased information processing is a factor in [[depression (mood)|depression]].<ref>{{Citation |first=Ivy-Marie |last=Blackburn |first2=Kate M. |last2=Davidson |year=1995 |title=Cognitive therapy for depression & anxiety: a practitioner's guide |publisher=Wiley-Blackwell |isbn=9780632039869 |oclc=32699443 |edition=2 |page=19}}</ref> His approach teaches people to treat evidence impartially, rather than selectively reinforcing negative outlooks.<ref name= "baron195"/> [[Phobias]] and [[hypochondria]] have also been shown to involve confirmation bias for threatening information.<ref>{{Citation |first=Allison G. |last=Harvey |first2=Edward |last2=Watkins |first3= Warren |last3=Mansell |year=2004 |title=Cognitive behavioural processes across psychological disorders: a transdiagnostic approach to research and treatment |publisher=Oxford University Press |isbn= 9780198528883 |oclc=602015097 |pages=172<U+2013>173,176}}</ref>

===In politics and law===
[[Image:Witness impeachment.jpg|thumb|right|alt=A woman and a man reading a document in a courtroom|[[Mock trial]]s allow researchers to examine confirmation biases in a realistic setting.]]
Nickerson argues that reasoning in judicial and political contexts is sometimes subconsciously biased, favoring conclusions that judges, juries or governments have already committed to.<ref>{{Harvnb|Nickerson |1998|pp=191<U+2013>193}}</ref> Since the evidence in a jury trial can be complex, and jurors often reach decisions about the verdict early on, it is reasonable to expect an attitude polarization effect. The prediction that jurors will become more extreme in their views as they see more evidence has been borne out in experiments with [[mock trial]]s.<ref>{{Citation |last=Myers |first=D.G. |first2=H. |last2=Lamm |year=1976 |title=The group polarization phenomenon |journal=Psychological Bulletin |volume=83 |pages=602<U+2013>627 |doi=10.1037/0033-2909.83.4.602}} via {{Harvnb|Nickerson|1998|pp=193<U+2013>194}}</ref><ref name="halpern">{{Citation |last=Halpern |first=Diane F. |title=Critical thinking across the curriculum: a brief edition of thought and knowledge |publisher=Lawrence Erlbaum Associates |year=1987 |page=194 |isbn=9780805827316 |oclc=37180929}}</ref>

Confirmation bias can be a factor in creating or extending conflicts, from emotionally charged debates to wars: by interpreting the evidence in their favor, each opposing party can become overconfident that it is in the stronger position.<ref name="baron191">{{Harvnb|Baron|2000|pp=191,195}}</ref> On the other hand, confirmation bias can result in people ignoring or misinterpreting the signs of an imminent or incipient conflict. For example, psychologists [[Stuart Sutherland]] and Thomas Kida have each argued that US Admiral [[Husband E. Kimmel]] showed confirmation bias when playing down the first signs of the Japanese [[attack on Pearl Harbor]].<ref name="sutherland"/><ref>{{Harvnb|Kida|2006|p=155}}</ref>

A two-decade study of political pundits by [[Philip E. Tetlock]] found that, on the whole, their predictions were not much better than chance. Tetlock divided experts into "foxes" who maintained multiple hypotheses, and "hedgehogs" who were more dogmatic. In general, the hedgehogs were much less accurate. Tetlock blamed their failure on confirmation bias<U+2014>specifically, their inability to make use of new information that contradicted their existing theories.<ref>{{Citation |last=Tetlock |first=Philip E. |title=Expert Political Judgment: How Good Is It? How Can We Know? |publisher=Princeton University Press |location=Princeton, N.J. |year=2005 |isbn=9780691123028 |oclc=56825108 |pages=125<U+2013>128}}</ref>

===In the paranormal===
One factor in the appeal of [[psychic]] "readings" is that listeners apply a confirmation bias which fits the psychic's statements to their own lives.<ref name="toolkit">{{Citation |last=Smith |first= Jonathan C. |title=Pseudoscience and Extraordinary Claims of the Paranormal: A Critical Thinker's Toolkit |publisher=John Wiley and Sons |year=2009 |pages=149<U+2013>151 |isbn=9781405181228 |oclc=319499491}}</ref> By making a large number of ambiguous statements in each sitting, the psychic gives the client more opportunities to find a match. This is one of the techniques of [[cold reading]], with which a psychic can deliver a subjectively impressive reading without any prior information about the client.<ref name="toolkit"/> Investigator [[James Randi]] compared the transcript of a reading to the client's report of what the psychic had said, and found that the client showed a strong selective recall of the "hits".<ref>{{Citation |last=Randi |first=James |title=James Randi: psychic investigator |publisher=Boxtree |year=1991 |isbn=9781852831448 |oclc= 26359284 |pages=58<U+2013>62}}</ref>

As a "striking illustration" of confirmation bias in the real world, Nickerson mentions numerological [[pyramidology]]: the practice of finding meaning in the proportions of the Egyptian pyramids.<ref name= nick190/> There are many different length measurements that can be made of, for example, the [[Great Pyramid of Giza]] and many ways to combine or manipulate them. Hence it is almost inevitable that people who look at these numbers selectively will find superficially impressive correspondences, for example with the dimensions of the Earth.<ref name="nick190">{{Harvnb|Nickerson|1998|p=190}}</ref>

===In scientific procedure===
A distinguishing feature of [[science|scientific thinking]] is the search for falsifying as well as confirming evidence.<ref name=nick194/> However, many times in the [[history of science]], scientists have resisted new discoveries by selectively interpreting or ignoring unfavorable data.<ref name="nick194">{{Harvnb|Nickerson|1998|pp=192<U+2013>194}}</ref> In the context of scientific research, confirmation biases can sustain theories or research programs in the face of inadequate or even contradictory evidence;<ref name="sutherland">{{Citation |last=Sutherland |first=Stuart |title=Irrationality |edition=2nd |publisher= Pinter and Martin |location=London |year=2007 |pages=95<U+2013>103 |isbn=9781905177073 |oclc=72151566}}</ref><ref>{{Citation |last=Proctor |first=Robert W. |first2=E. John |last2=Capaldi |title=Why science matters: understanding the methods of psychological research |publisher=Wiley-Blackwell |year=2006 |page=68 |isbn=9781405130493 |oclc=318365881}}</ref> the field of [[parapsychology]] has been particularly affected by confirmation bias in this way.<ref>{{Citation |last=Sternberg |first=Robert J. |editor1-first=Robert J. |editor1-last=Sternberg |editor2-first=Henry L. |editor2-last=Roediger III |editor3-first=Diane F. |editor3-last=Halpern |title=Critical Thinking in Psychology |year=2007 |publisher=Cambridge University Press |isbn=0521608341 |oclc=69423179 |page=292 |chapter=Critical Thinking in Psychology: It really is critical |quote=Some of the worst examples of confirmation bias are in research on parapsychology (...) Arguably, there is a whole field here with no powerful confirming data at all. But people want to believe, and so they find ways to believe.}}</ref> An experimenter's confirmation bias can potentially affect which data are reported. Data that conflict with the experimenter's expectations may be more readily discarded as unreliable, producing the so-called [[publication bias|file drawer effect]]. To combat this tendency, scientific training teaches ways to avoid bias.<ref name="shadish">{{Citation |last=Shadish |first=William R. |title= Critical Thinking in Psychology |editor1-first=Robert J. |editor1-last=Sternberg |editor2-first=Henry L. |editor2-last=Roediger III |editor3-first=Diane F. |editor3-last=Halpern |publisher=Cambridge University Press |year=2007 |page=49 |chapter=Critical Thinking in Quasi-Experimentation |isbn=9780521608343}}</ref> [[Experimental design]]s involving [[Randomization#Randomized experiments|randomization]] and [[double blind trials]], along with the social process of [[peer review]], mitigate the effect of individual scientists' bias.<ref name="shadish"/><ref>{{Citation |last=Shermer |first=Michael |date=July 2006 |title=The Political Brain |journal=Scientific American |url=http://www.scientificamerican.com/article.cfm?id=the-political-brain |issn=0036-8733 |accessdate=2009-08-14}}</ref>

===In self-image===
Social psychologists have identified two processes in the way people seek or interpret information about themselves that are served by confirmation biases: ''[[self-verification]]'', the drive to reinforce the existing self-image, and ''[[self-enhancement]]'', the tendency to seek positive feedback.<ref name="reconciling">{{Citation |last=Swann |first=William B. |first2=Brett W. |last2=Pelham |first3= Douglas S. |last3=Krull |title=Agreeable Fancy or Disagreeable Truth? Reconciling Self-Enhancement and Self-Verification |journal=Journal of Personality and Social Psychology |year=1989 |volume=57 |issue=5 |pages=782<U+2013>791 |publisher=American Psychological Association |issn=0022<U+2013>3514}}</ref> In experiments where people are given feedback that conflicts with their self-image, they are less likely to attend to it or remember it than when given self-verifying feedback.<ref name="swannread_jesp"/><ref>{{Citation |last=Story |first=Amber L. |title=Self-Esteem and Memory for Favorable and Unfavorable Personality Feedback |journal=Personality and Social Psychology Bulletin |year=1998 |volume=24 |issue=1 |pages= 51<U+2013>64 |doi=10.1177/0146167298241004 |accessdate=18 May 2010 |publisher=Society for Personality and Social Psychology |issn=1552-7433}}</ref><ref>{{Citation |last=White |first=Michael J. |first2=Daniel R. |last2 =Brockett |first3=Belinda G. |last3=Overstreet |title=Confirmatory Bias in Evaluating Personality Test Information: Am I Really That Kind of Person? |journal=Journal of Counseling Psychology |year=1993 |volume= 40 |issue=1 |pages=120<U+2013>126 |doi=10.1037/0022-0167.40.1.120 |publisher=American Psychological Association |issn=0022-0167}}</ref> They reduce the impact of such information by interpreting it as unreliable.<ref name="swannread_jesp">{{Citation |last=Swann |first=William B. |first2=Stephen J. |last2=Read |title=Self-Verification Processes: How We Sustain Our Self-Conceptions |journal=Journal of Experimental Social Psychology |year=1981 |volume=17 |issue=4 |pages=351<U+2013>372 |publisher=Academic Press |issn=0022<U+2013>1031 |doi=10.1016/0022-1031(81)90043-3}}</ref><ref name="swannread_jpsp">{{Citation |last=Swann |first=William B. |first2=Stephen J. |last2=Read |title=Acquiring Self-Knowledge: The Search for Feedback That Fits |journal=Journal of Personality and Social Psychology |year=1981 |volume=41 |issue=6 |pages=1119<U+2013>1128 |publisher =American Psychological Association |issn=0022<U+2013>3514}}</ref><ref>{{Citation |last=Shrauger |first=J. Sidney |first2=Adrian K. |last2=Lund |title=Self-evaluation and reactions to evaluations from others |journal= Journal of Personality |year=1975 |volume=43 |issue=1 |pages=94<U+2013>108 |doi=10.1111/j.1467-6494.1975.tb00574 |accessdate=18 May 2010 |publisher=Duke University Press}}</ref> Similar experiments have found a preference for positive feedback, and the people who give it, over negative feedback.<ref name="reconciling"/>

==See also==
{{Portal box|Psychology|Thinking}}
<div style="-moz-column-count:2;">
* [[List of cognitive biases]]
* [[List of memory biases]]
* [[Observer-expectancy effect]]
* [[Selective exposure theory]]
</div>

==Footnotes==
{{reflist|group="Note"}}

==References==
===Notes===
{{Reflist|2}}

===Sources===
* {{Citation |last=Baron |first=Jonathan |year=2000 |title=Thinking and deciding |edition=3rd |location=New York |publisher=Cambridge University Press |isbn=0521650305 |oclc=316403966 |ref=harv}}
* {{Citation |last=Fine |first=Cordelia |title=A Mind of its Own: how your brain distorts and deceives |publisher=Icon books |location=Cambridge, UK |year=2006 |isbn=1840466782 |oclc=60668289 |ref=harv}}
* {{Citation |last=Friedrich |first=James |title=Primary error detection and minimization (PEDMIN) strategies in social cognition: a reinterpretation of confirmation bias phenomena |journal=Psychological Review |year=1993 |volume=100 |issue=2 |pages=298<U+2013>319 |pmid=8483985 |publisher=American Psychological Association |issn=0033-295X |ref=harv}}
* {{Citation |last=Goldacre |first=Ben |title=Bad Science |publisher=Fourth Estate |location=London |year=2008 |isbn=9780007240197 |oclc=259713114 |ref=harv}}
* {{Citation |last=Kida |first=Thomas |title=Don't Believe Everything You Think: The 6 Basic Mistakes We Make in Thinking |publisher=Prometheus Books |year=2006 |isbn=9781591024088 |oclc=63297791 |ref=harv}}
* {{Citation |last=Kunda |first=Ziva |title=Social Cognition: Making Sense of People |publisher=MIT Press |year=1999 |isbn=9780262611435 |oclc=40618974 |ref=harv}}
* {{Citation |last=Lewicka |first=Maria |editor1-first=Miros<U+0142>aw |editor1-last=Kofta |editor2-first=Gifford |editor2-last=Weary |editor3-first= Grzegorz |editor3-last=Sedek |title=Personal control in action: cognitive and motivational mechanisms |publisher=Springer |year=1998 |isbn=9780306457203 |oclc=39002877 |chapter=Confirmation Bias: Cognitive Error or Adaptive Strategy of Action Control? |pages=233<U+2013>255 |ref= harv}}
* {{Citation |last=Oswald |first=Margit E. |first2=Stefan |last2=Grosjean |title=Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory |editor-first=R<U+00FC>diger F. |editor-last= Pohl |publisher=Psychology Press |location=Hove, UK |year=2004 |chapter=Confirmation Bias |isbn=9781841693514 |oclc=55124398 |pages=79<U+2013>96 |ref=harv}}
* {{Citation |last=Nickerson |first=Raymond S. |year=1998 |title=Confirmation Bias; A Ubiquitous Phenomenon in Many Guises |journal=Review of General Psychology |publisher=Educational Publishing Foundation |volume=2 |issue=2 |pages=175<U+2013>220 |issn=1089-2680 |doi=10.1037/1089-2680.2.2.175}}
* {{Citation |last=Plous |first=Scott |title=The Psychology of Judgment and Decision Making |publisher=McGraw-Hill |year=1993  |isbn=9780070504776 |oclc=26931106 |ref=harv}}
* {{Citation |last=Poletiek |first=Fenna |title= Hypothesis-testing behaviour |publisher=Psychology Press |location=Hove, UK |year=2001 |isbn=9781841691596 |oclc=44683470|ref=harv}}
* {{Citation |last=Risen |first=Jane |first2=Thomas |last2= Gilovich |title=Critical Thinking in Psychology |editor1-first=Robert J. |editor1-last=Sternberg |editor2-first=Henry L. |editor2-last=Roediger III |editor3-first=Diane F. |editor3-last=Halpern |publisher=  Cambridge University Press |year=2007 |pages=110<U+2013>130 |chapter=Informal Logical Fallacies |isbn=9780521608343 |oclc=69423179|ref=harv}}
* {{Citation |last=Vyse |first=Stuart A. |title=Believing in magic: The psychology of superstition |year=1997 |isbn=0195136349 |location=New York |publisher=Oxford University Press |oclc=35025826 |ref=harv}}

==Further reading==
* {{Citation |last=Westen |first=Drew |title=The political brain: the role of emotion in deciding the fate of the nation |publisher=PublicAffairs |year=2007 |isbn=9781586484255 |oclc=86117725}}

==External links==
* [http://skepdic.com/confirmbias.html Skeptic's Dictionary: confirmation bias] by Robert T. Carroll
* [http://www.devpsy.org/teaching/method/confirmation_bias.html Teaching about confirmation bias], class handout and instructor's notes by K. H. Grobman
* [http://hosted.xamai.ca/confbias/ Confirmation bias learning object], interactive number triples exercise by Rod McFarland, Simon Fraser University
* [http://faculty.babson.edu/krollag/org_site/soc_psych/lord_death_pen.html Brief summary of the 1979 Stanford assimilation bias study] by Keith Rollag, Babson College
* [http://www.talkorigins.org/origins/postmonth/feb02.html "Morton's demon"], Usenet post by Glenn Morton, February 2, 2002<!--cited on page 12 of Mark Isaak  (2007). ''The counter-creationism handbook''. University of California Press. isbn 9780520249264-->

{{Biases}}
{{DEFAULTSORT:Confirmation Bias}}
{{featured article}}
[[Category:Bias]]
[[Category:Cognitive biases]]
[[Category:Critical thinking]]
[[Category:Experimental design]]
[[Category:Inductive fallacies]]
[[Category:Logical fallacies]]
[[Category:Misuse of statistics]]

[[ca:Biaix de confirmaci<U+00F3>]]
[[de:Best<U+00E4>tigungsfehler]]
[[es:Sesgo de confirmaci<U+00F3>n]]
[[fr:Biais de confirmation d'hypoth<U+00E8>se]]
[[ko:<U+D655><U+C99D><U+D3B8><U+D5A5>]]
[[is:Sta<U+00F0>festingartilhneiging]]
[[he:<U+05D4><U+05D8><U+05D9><U+05D9><U+05EA> <U+05D0><U+05D9><U+05E9><U+05D5><U+05E8>]]
[[ja:<U+78BA><U+8A3C><U+30D0><U+30A4><U+30A2><U+30B9>]]
[[pl:Efekt potwierdzania]]
[[sv:Konfirmeringsbias]]
