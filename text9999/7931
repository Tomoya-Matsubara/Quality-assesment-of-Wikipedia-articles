[[Image:Consistency of estimator.svg|thumb|250px|{''T''<sub>1</sub>, ''T''<sub>2</sub>, ''T''<sub>3</sub>, <U+2026>} is a sequence of estimators  for parameter ''<U+03B8>''<sub>0</sub> (the true value of which is 4). This sequence is consistent: the estimators are getting more and more concentrated near the true value ''<U+03B8>''<sub>0</sub>. The limiting distribution of the sequence is a degenerate random variable which equals ''<U+03B8>''<sub>0</sub> with probability 1.]]

In [[statistics]], a sequence of [[estimator]]s for parameter ''<U+03B8>''<sub>0</sub> is said to be '''consistent''' (or '''asymptotically consistent''') if this sequence [[convergence in probability|converges in probability]] to ''<U+03B8>''<sub>0</sub>. It means that the distributions of the estimators become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to ''<U+03B8>''<sub>0</sub> converges to one.

In practice one usually constructs a single estimator as a function of an available sample of [[sample size|size]] ''n'', and then imagines being able to keep collecting data and expanding the sample ''ad infinitum''. In this way one would obtain a sequence of estimators indexed by ''n'' and the notion of consistency will be understood as the sample size <U+201C>grows to infinity<U+201D>. If this sequence converges in probability to the true value ''<U+03B8>''<sub>0</sub>, we call it a consistent estimator; otherwise the estimator is said to be '''inconsistent'''.

Consistency as defined here is sometimes referred to as ''weak consistency''. When we replace convergence in probability with [[almost sure convergence]], then the sequence of estimators is said to be ''strongly consistent''.

== Definition ==
Loosely speaking, an [[estimator]] ''T<sub>n</sub>'' of ''<U+03B8>'' is said to be '''consistent''' if it [[convergence in probability|converges in probability]] to the true value of the parameter:<ref>{{harvnb|Amemiya|1985|loc=Definition 3.4.2}}</ref>
: <math>
    \underset{n\to\infty}{\operatorname{plim}}\ T_n = \theta.
  </math>

More rigorous definition takes into account the fact that ''<U+03B8>'' is actually unknown, and thus the convergence in probability must take place for every possible value of this parameter. Let {{nowrap|{''p<sub><U+03B8></sub>'': ''<U+03B8>''&thinsp;<U+2208>&thinsp;<U+0398>}}} be a family of distributions (called the [[parametric model]]), and let {{nowrap|1=''X<sup><U+03B8></sup>'' = {''X''<sub>1</sub>, ''X''<sub>2</sub>, <U+2026> : ''X<sub>i</sub>'' ~ ''p<sub><U+03B8></sub>''}}} denote an infinite [[statistical sample|sample]] from the distribution ''p<sub><U+03B8></sub>''. Suppose {''T<sub>n</sub>''(''X<sup><U+03B8></sup>'')} is a sequence of estimators for some parameter ''g''(''<U+03B8>''). Usually ''T<sub>n</sub>'' will be based on the first ''n'' observations of a sample. Then this sequence {''T<sub>n</sub>''} is said to be (weakly) '''consistent''' if <ref>{{harvnb|Lehman|Casella|1998|page=332}}</ref>
: <math>
    \underset{n\to\infty}{\operatorname{plim}}\ T_n(X^{\theta}) = g(\theta),\ \ \text{for all}\ \theta\in\Theta.
  </math> 

This definition uses ''g''(''<U+03B8>'') instead of simply ''<U+03B8>'', because often one is interested in estimating a certain function or a sub-vector of the underlying parameter. In the next example we estimate the location parameter of the model, but not the scale:

=== Example: sample mean for normal random variables ===

Suppose one has a sequence of observations {''X''<sub>1</sub>, ''X''<sub>2</sub>, <U+2026>} from a [[Normal distribution|normal ''N''(''<U+03BC>'',&thinsp;''<U+03C3>''<sup>2</sup>)]] distribution. To estimate ''<U+03BC>'' based on the first ''n'' observations, we use the sample mean: ''T<sub>n</sub>''&nbsp;=&nbsp;(''X''<sub>1</sub> + <U+2026> + ''X<sub>n</sub>'')/''n''. This defines a sequence of estimators, indexed by the sample size ''n''.

From the properties of the normal distribution, we know that ''T''<sub>''n''</sub> is itself normally distributed, with mean ''<U+03BC>'' and variance ''<U+03C3>''<sup>2</sup>/''n''. Equivalently, <math style="vertical-align:-.3em">\scriptstyle (T_n-\mu)/(\sigma/\sqrt{n})</math> has a standard normal distribution. Then
: <math>
    \Pr\!\Big[\,|T_n-\mu|\geq\varepsilon\,\Big] = 
    \Pr\!\left[ \sqrt{n}\big|T_n-\mu\big|/\sigma \geq \sqrt{n}\varepsilon/\sigma \right] = 
    2\big(1-\Phi(\sqrt{n}\varepsilon/\sigma)\big)\ \to\ 0
  </math>
as ''n'' tends to infinity, for any fixed {{nowrap|''<U+03B5>'' > 0}}. Therefore, the sequence ''T<sub>n</sub>'' of sample means is consistent for the population mean ''<U+03BC>''.

== Establishing consistency ==

The notion of asymptotic consistency is very close, almost synonymous to the notion of convergence in probability. As such, any theorem, lemma, or property which establishes convergence in probability may be used to prove the consistency. Many such tools exist:

<ul>

<li>In order to demonstrate consistency directly from the definition one can use the inequality <ref>{{harvnb|Amemiya|1985|loc=equation (3.2.5)}}</ref>
: <math>
    \Pr\!\big[h(T_n-\theta)\geq\varepsilon\big] \leq \frac{\operatorname{E}\big[h(T_n-\theta)\big]}{\varepsilon},
  </math>
the most common choice for function ''h'' being either the absolute value (in which case it is known as [[Markov inequality]]), or the quadratic function (respectively [[Chebychev's inequality]]).

<li>Another useful result is the [[continuous mapping theorem]]: if ''T<sub>n</sub>'' is consistent for ''<U+03B8>'' and ''g''(<U+00B7>) is a real-valued function continuous at point ''<U+03B8>'', then ''g''(''T<sub>n</sub>'') will be consistent for ''g''(''<U+03B8>''): <ref>{{harvnb|Amemiya|1985|loc=Theorem 3.2.6}}</ref>
: <math>
    T_n\ \xrightarrow{p}\ \theta\ \quad\Rightarrow\quad g(T_n)\ \xrightarrow{p}\ g(\theta)
  </math>

<li>[[Slutsky<U+2019>s theorem]] can be used to combine several different estimators, or an estimator with a non-random covergent sequence. If ''T<sub>n</sub>''&nbsp;<U+2192><sup style="position:relative;top:-.2em;left:-1em;">''p''</sup>''<U+03B1>'', and ''S<sub>n</sub>''&nbsp;<U+2192><sup style="position:relative;top:-.2em;left:-1em;">''p''</sup>''<U+03B2>'', then <ref>{{harvnb|Amemiya|1985|loc=Theorem 3.2.7}}</ref>
: <math>\begin{align}
  & T_n + S_n \ \xrightarrow{p}\ \alpha+\beta, \\
  & T_n   S_n \ \xrightarrow{p}\ \alpha \beta, \\
  & T_n / S_n \ \xrightarrow{p}\ \alpha/\beta,\ \text{provided that}\ \beta\neq0
  \end{align}</math>

<li>If estimator ''T<sub>n</sub>'' is given by an explicit formula, then most likely the formula will employ sums of random variables, and then the [[law of large numbers]] can be used: for a sequence {''X<sub>n</sub>''} of random variables and under suitable conditions,
: <math>\frac{1}{n}\sum_{i=1}^n g(X_i) \ \xrightarrow{p}\ \operatorname{E}[\,g(X)\,]</math>

<li>If estimator ''T<sub>n</sub>'' is defined implicitly, for example as a value that maximizes certain objective function (see [[extremum estimator]]), then a more complicated argument involving [[stochastic equicontinuity]] has to be used. <ref>{{harvtxt|Newey|McFadden|1994|loc=Chapter 2}}</ref>

</ul>

==See also==
* [[Fisher consistency]] <U+2014> alternative, although rarely used concept of consistency for the estimators
* [[Statistical hypothesis testing|Consistent test]] <U+2014> the notion of consistency in the context of hypothesis testing

== Notes ==
{{reflist}}

== References ==
{{refbegin}}
* {{cite book
  | last = Amemiya | first = Takeshi | authorlink = Takeshi Amemiya
  | title = Advanced econometrics
  | year = 1985
  | publisher = Harvard University Press
  | isbn = 0-674-00560-0
  | ref = CITEREFAmemiya1985
  }}
* {{cite book
  | last1 = Lehmann | first1 = E. L.
  | last2 = Casella | first2 = G.
  | title = Theory of Point Estimation
  | year = 1998
  | edition = 2nd
  | publisher = Springer
  | isbn = 0-387-98502-6
  | ref = CITEREFLehmanCasella1998
  }}
* {{cite book
  | last1 = Newey | first1 = W.
  | last2 = McFadden | first2 = D. | authorlink2 = Daniel McFadden
  | title = Large sample estimation and hypothesis testing
  | year = 1994
  | series = In <U+201C>Handbook of Econometrics<U+201D>, Vol. 4, Ch. 36
  | publisher = Elsevier Science
  | isbn = 0-444-88766-0
  | ref = CITEREFNeweyMcFadden1994
  }}
* {{SpringerEOM| title=Consistent estimator |id=C/c025240 |first=M.S. |last=Nikulin}}
{{refend}}

{{DEFAULTSORT:Consistent estimator}}
[[Category:Statistical theory]]
[[Category:Statistical inference]]
[[Category:Estimation theory]]

[[de:Konsistenz (Statistik)]]
[[es:Consistencia (estad<U+00ED>stica)]]
[[ru:<U+0421><U+043E><U+0441><U+0442><U+043E><U+044F><U+0442><U+0435><U+043B><U+044C><U+043D><U+0430><U+044F> <U+043E><U+0446><U+0435><U+043D><U+043A><U+0430>]]
[[uk:<U+041A><U+043E><U+043D><U+0437><U+0438><U+0441><U+0442><U+0435><U+043D><U+0442><U+043D><U+0430> <U+043E><U+0446><U+0456><U+043D><U+043A><U+0430>]]
